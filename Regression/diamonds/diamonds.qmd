---
title: "Diamond Price Prediction"
format:
  html:
    code-fold: true
    code-tools: true
jupyter: python3
---

```{python}
# | label: diamond-price-prediction
# | warning: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import lightgbm as lgb
import catboost as cb
import joblib
import re


# Function to clean feature names
def clean_feature_name(name):
    return re.sub(r"[^A-Za-z0-9_]+", "_", name)


# Load the dataset
diamonds = pd.read_csv("diamonds.csv")

# Display the first few rows and basic information about the dataset
print(diamonds.head())
print(diamonds.info())

# Visualize the distribution of the target variable
plt.figure(figsize=(10, 6))
sns.histplot(diamonds["price"], kde=True)
plt.title("Distribution of Diamond Prices")
plt.show()

# Check correlations for numerical columns only
numerical_columns = diamonds.select_dtypes(include=[np.number]).columns
plt.figure(figsize=(12, 10))
sns.heatmap(diamonds[numerical_columns].corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap (Numerical Features)")
plt.show()

# Check for missing values
print(diamonds.isnull().sum())

# Select numerical features for scaling
numerical_features = ["carat", "depth", "table", "x", "y", "z"]

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the numerical features
diamonds[numerical_features] = scaler.fit_transform(diamonds[numerical_features])

# Split the data into features (X) and target (y)
X = diamonds.drop("price", axis=1)
y = diamonds["price"]

# Perform one-hot encoding for categorical variables
X = pd.get_dummies(X, columns=["cut", "color", "clarity"], drop_first=True)

# Clean feature names
X.columns = [clean_feature_name(col) for col in X.columns]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# Function to evaluate models
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    return mse, r2


# Linear Regression
lr = LinearRegression()
lr_mse, lr_r2 = evaluate_model(lr, X_train, X_test, y_train, y_test)

# Ridge Regression
ridge = Ridge(alpha=1.0)
ridge_mse, ridge_r2 = evaluate_model(ridge, X_train, X_test, y_train, y_test)

# Lasso Regression
lasso = Lasso(alpha=1.0)
lasso_mse, lasso_r2 = evaluate_model(lasso, X_train, X_test, y_train, y_test)

# ElasticNet
elastic = ElasticNet(alpha=1.0, l1_ratio=0.5)
elastic_mse, elastic_r2 = evaluate_model(elastic, X_train, X_test, y_train, y_test)

# Random Forest
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf_mse, rf_r2 = evaluate_model(rf, X_train, X_test, y_train, y_test)

# XGBoost
xgb_model = xgb.XGBRegressor(random_state=42)
xgb_mse, xgb_r2 = evaluate_model(xgb_model, X_train, X_test, y_train, y_test)

# LightGBM
lgb_model = lgb.LGBMRegressor(random_state=42)
lgb_mse, lgb_r2 = evaluate_model(lgb_model, X_train, X_test, y_train, y_test)

# CatBoost
cb_model = cb.CatBoostRegressor(random_state=42, verbose=0)
cb_mse, cb_r2 = evaluate_model(cb_model, X_train, X_test, y_train, y_test)

# Print results for all models
print("Linear Regression - MSE: {:.2f}, R2: {:.2f}".format(lr_mse, lr_r2))
print("Ridge Regression - MSE: {:.2f}, R2: {:.2f}".format(ridge_mse, ridge_r2))
print("Lasso Regression - MSE: {:.2f}, R2: {:.2f}".format(lasso_mse, lasso_r2))
print("ElasticNet - MSE: {:.2f}, R2: {:.2f}".format(elastic_mse, elastic_r2))
print("Random Forest - MSE: {:.2f}, R2: {:.2f}".format(rf_mse, rf_r2))
print("XGBoost - MSE: {:.2f}, R2: {:.2f}".format(xgb_mse, xgb_r2))
print("LightGBM - MSE: {:.2f}, R2: {:.2f}".format(lgb_mse, lgb_r2))
print("CatBoost - MSE: {:.2f}, R2: {:.2f}".format(cb_mse, cb_r2))

# Compare model performance
models = [
    "Linear Regression",
    "Ridge",
    "Lasso",
    "ElasticNet",
    "Random Forest",
    "XGBoost",
    "LightGBM",
    "CatBoost",
]
r2_scores = [lr_r2, ridge_r2, lasso_r2, elastic_r2, rf_r2, xgb_r2, lgb_r2, cb_r2]

plt.figure(figsize=(10, 6))
sns.barplot(x=models, y=r2_scores)
plt.title("Model Comparison - R2 Scores")
plt.xticks(rotation=45)
plt.show()

# Feature importance for the Random Forest model
feature_importance = pd.DataFrame(
    {"feature": X.columns, "importance": rf.feature_importances_}
)
feature_importance = feature_importance.sort_values("importance", ascending=False).head(
    10
)

plt.figure(figsize=(10, 6))
sns.barplot(x="importance", y="feature", data=feature_importance)
plt.title("Top 10 Feature Importance - Random Forest")
plt.show()

# Hyperparameter tuning for Random Forest
param_grid = {
    "n_estimators": [100, 200, 300],
    "max_depth": [10, 20, 30, None],
    "min_samples_split": [2, 5, 10],
}

rf_grid = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2,
)

rf_grid.fit(X_train, y_train)

print("Best parameters: ", rf_grid.best_params_)
print("Best score: ", rf_grid.best_score_)

rf_tuned = rf_grid.best_estimator_
rf_tuned_mse, rf_tuned_r2 = evaluate_model(rf_tuned, X_train, X_test, y_train, y_test)
print("Tuned Random Forest - MSE: {:.2f}, R2: {:.2f}".format(rf_tuned_mse, rf_tuned_r2))

# Cross-validation
cv_scores = cross_val_score(rf_tuned, X, y, cv=5, scoring="r2")

print("Cross-validation scores:", cv_scores)
print(
    "Mean R2 score: {:.2f} (+/- {:.2f})".format(cv_scores.mean(), cv_scores.std() * 2)
)

# Train final model and save
final_model = rf_tuned.fit(X, y)
joblib.dump(final_model, "diamond_price_prediction_model.joblib")

print("Final model has been trained and saved.")

# Analyze categorical columns
categorical_columns = diamonds.select_dtypes(include=["object"]).columns
for col in categorical_columns:
    plt.figure(figsize=(10, 6))
    sns.boxplot(x=col, y="price", data=diamonds)
    plt.title(f"Price Distribution by {col}")
    plt.xticks(rotation=45)
    plt.show()

    print(f"\nUnique values in {col}:")
    print(diamonds[col].value_counts())
    print("\n")
