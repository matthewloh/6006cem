Certainly! I'd be happy to help you implement the regression notebook in Quarto format with detailed explanations. Here's a high-level outline of how we can structure the Quarto document:
{python}
Data Handling
import pandas as pd
import numpy as np
Modeling
import lightgbm as lgb
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder
Visualization
import matplotlib.pyplot as plt
import seaborn as sns
Load the data
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')
print(train.shape)
print(test.shape)
train.head()
{python}
Function to preprocess data
def preprocess_data(df):
# Handle missing values
for col in df.columns:
if df[col].dtype != 'object':
df[col].fillna(df[col].median(), inplace=True)
else:
df[col].fillna(df[col].mode()[0], inplace=True)
# Encode categorical variables
le = LabelEncoder()
for col in df.select_dtypes(include=['object']).columns:
df[col] = le.fit_transform(df[col].astype(str))
return df
Preprocess train and test data
train_processed = preprocess_data(train.drop('SalePrice', axis=1))
test_processed = preprocess_data(test)
X = train_processed
y = train['SalePrice']
print("Processed data shape:", X.shape)
{python}
def train_lightgbm(X, y, test_data, n_folds=5):
# Split data
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
# Define LightGBM parameters
params = {
'objective': 'regression',
'metric': 'rmse',
'num_leaves': 31,
'learning_rate': 0.05,
'feature_fraction': 0.9
}
# Train model with cross-validation
kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)
models = []
for train_index, val_index in kf.split(X_train):
X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]
y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]
lgb_train = lgb.Dataset(X_train_fold, y_train_fold)
lgb_val = lgb.Dataset(X_val_fold, y_val_fold, reference=lgb_train)
model = lgb.train(params, lgb_train, num_boost_round=1000, valid_sets=[lgb_train, lgb_val],
early_stopping_rounds=100, verbose_eval=100)
models.append(model)
return models
Train the model
models = train_lightgbm(X, y, test_processed)
{python}
def evaluate_model(models, X, y):
predictions = np.zeros(len(X))
for model in models:
predictions += model.predict(X)
predictions /= len(models)
mse = mean_squared_error(y, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(y, predictions)
print(f"RMSE: {rmse}")
print(f"R2 Score: {r2}")
plt.figure(figsize=(10, 6))
plt.scatter(y, predictions)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted House Prices')
plt.show()
evaluate_model(models, X, y)
{python}
def plot_feature_importance(models, X):
importance_df = pd.DataFrame()
for model in models:
importance_df = importance_df.append(pd.DataFrame({'feature': X.columns,
'importance': model.feature_importance()})
.sort_values('importance', ascending=False))
importance_df = importance_df.groupby('feature').mean().sort_values('importance', ascending=False)
plt.figure(figsize=(10, 6))
sns.barplot(x=importance_df.index[:20], y=importance_df['importance'][:20])
plt.xticks(rotation=90)
plt.title('Top 20 Most Important Features')
plt.tight_layout()
plt.show()
plot_feature_importance(models, X)
{python}
def make_predictions(models, test_data):
predictions = np.zeros(len(test_data))
for model in models:
predictions += model.predict(test_data)
predictions /= len(models)
return predictions
test_predictions = make_predictions(models, test_processed)
submission = pd.DataFrame({'Id': test['Id'], 'SalePrice': test_predictions})
submission.to_csv('submission.csv', index=False)
print(submission.head())
This Quarto document provides a comprehensive implementation of the regression task, including detailed explanations at each step. It covers data preprocessing, model training with cross-validation, evaluation, feature importance analysis, and making predictions on the test set.
The code is structured to be modular and reusable, with functions defined for key tasks. This allows for easy experimentation and modification.
To further improve this implementation, you could consider:
1. Adding more exploratory data analysis (EDA) visualizations
Implementing more advanced feature engineering techniques
Using Bayesian optimization for hyperparameter tuning
Experimenting with other algorithms like XGBoost or CatBoost for comparison
Implementing stacking or blending of multiple models
Remember to adjust the code as needed based on your specific dataset and requirements. Good luck with your regression task!