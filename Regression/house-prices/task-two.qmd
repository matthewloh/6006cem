---
title: "House Prices Regression Task - Beginner's Approach"
format:

  pdf:
    toc: true
    colorlinks: true
jupyter: python3
---

## Importing Libraries

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import LabelEncoder

# Set random seed for reproducibility
np.random.seed(42)
```

This code imports necessary libraries. pandas and numpy are for data manipulation, matplotlib and seaborn for visualization, and scikit-learn modules for machine learning tasks.

## Data Loading and Exploratory Data Analysis

```{python}
# Load the data
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

# Display basic information about the dataset
print(train.info())

# Display summary statistics of numerical columns
print(train.describe())

# Plot distribution of target variable (SalePrice)
plt.figure(figsize=(10, 6))
sns.histplot(train["SalePrice"], kde=True)
plt.title("Distribution of Sale Prices")
plt.show()

# Scatter plot of GrLivArea vs SalePrice
plt.figure(figsize=(10, 6))
plt.scatter(train['GrLivArea'], train['SalePrice'], alpha=0.5)
plt.title('GrLivArea vs SalePrice')
plt.xlabel('GrLivArea (Above ground living area)')
plt.ylabel('SalePrice')
plt.show()

# Box plot of SalePrice by OverallQual
plt.figure(figsize=(12, 6))
sns.boxplot(x='OverallQual', y='SalePrice', data=train)
plt.title('SalePrice by Overall Quality')
plt.show()

# Correlation heatmap of numerical features
numeric_features = train.select_dtypes(include=[np.number])
plt.figure(figsize=(12, 10))
sns.heatmap(numeric_features.corr(), cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap of Numerical Features")
plt.show()
```

This section loads the data and performs basic exploratory data analysis. It includes visualizations of the target variable distribution, relationship between key features, and correlation between numerical features.

## Data Preprocessing

```{python}
def preprocess_data(df):
    # Handle missing values
    for col in df.columns:
        if df[col].dtype != "object":
            df[col] = df[col].fillna(df[col].median())
        else:
            df[col] = df[col].fillna(df[col].mode()[0])

    # Encode categorical variables
    le = LabelEncoder()
    for col in df.select_dtypes(include=["object"]).columns:
        df[col] = le.fit_transform(df[col].astype(str))

    return df

# Preprocess train and test data
X = preprocess_data(train.drop("SalePrice", axis=1))
y = train["SalePrice"]
test_processed = preprocess_data(test)

print("Processed data shape:", X.shape)
```

This code preprocesses the data by handling missing values and encoding categorical variables. It uses median imputation for numerical features and mode imputation for categorical features.

## Model Training and Evaluation

```{python}
# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train and evaluate Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_train_pred = lr_model.predict(X_train)
lr_val_pred = lr_model.predict(X_val)

print("Linear Regression Results:")
print(f"Train R2 Score: {r2_score(y_train, lr_train_pred):.4f}")
print(f"Validation R2 Score: {r2_score(y_val, lr_val_pred):.4f}")
print(f"Validation RMSE: {np.sqrt(mean_squared_error(y_val, lr_val_pred)):.4f}")

# Train and evaluate Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
rf_train_pred = rf_model.predict(X_train)
rf_val_pred = rf_model.predict(X_val)

print("\nRandom Forest Results:")
print(f"Train R2 Score: {r2_score(y_train, rf_train_pred):.4f}")
print(f"Validation R2 Score: {r2_score(y_val, rf_val_pred):.4f}")
print(f"Validation RMSE: {np.sqrt(mean_squared_error(y_val, rf_val_pred)):.4f}")

# Plot actual vs predicted values for both models
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_val, lr_val_pred, alpha=0.5)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel("Actual SalePrice")
plt.ylabel("Predicted SalePrice")
plt.title("Linear Regression: Actual vs Predicted")

plt.subplot(1, 2, 2)
plt.scatter(y_val, rf_val_pred, alpha=0.5)
plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)
plt.xlabel("Actual SalePrice")
plt.ylabel("Predicted SalePrice")
plt.title("Random Forest: Actual vs Predicted")

plt.tight_layout()
plt.show()
```

This section trains and evaluates two models: Linear Regression and Random Forest. It calculates R2 score and RMSE for both models and visualizes their performance using actual vs predicted plots.

## Feature Importance (Random Forest)

```{python}
# Get feature importances from Random Forest
importances = rf_model.feature_importances_
feature_imp = pd.DataFrame({'feature': X.columns, 'importance': importances})
feature_imp = feature_imp.sort_values('importance', ascending=False).head(10)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='importance', y='feature', data=feature_imp)
plt.title('Top 10 Feature Importances - Random Forest')
plt.tight_layout()
plt.show()
```

This code extracts and visualizes the top 10 most important features according to the Random Forest model.

## Predictions on Test Data

```{python}
# Make predictions on the test data using Random Forest
test_predictions = rf_model.predict(test_processed)

# Create submission file
submission = pd.DataFrame({'Id': test['Id'], 'SalePrice': test_predictions})
submission.to_csv('submission.csv', index=False)
print("Submission file created.")
```

This final section uses the Random Forest model to make predictions on the test data and creates a submission file.